PCA = Principle Components Analysis

1st step: calculate the means of objects by each feature (store in M)
2nd step: create Center matrix by subtract origin matrix (A) by M, i.e Center = A - M
3rd step: calculate the covariance matrix for Center matrix (store in Cov)
4th step: do the eigendecomposition for covariance matrix Cov, assuming hav m eigenvectors as well as eigenvalues
5th step: choose the k<m eigenvectors corresponding to k<m biggest eigenvalues
6th step: project the Center matrix into k-dimension by multiplying Center.dot(k-eigenvectors)

------------------------------------------------
In the 4th step, can use SVD decomposition instead of eigendecomposition in some cases

#-----------------------------------------------------------------------#
#------------------Calculate PCA manually-------------------------------#
#-----------------------------------------------------------------------#
matrix A (100,3) whose values range from 1 to 255

Matrix A(100, 3) =
       0    1    2
0    38  236  141
1    73  138  204
2   134   80  193
3   145  130  205
4    72  238  253
..  ...  ...  ...
95   75  137  110
96  100   33    9
97   85  206   51
98   80  170   65
99  109  212   25

[100 rows x 3 columns]

Center = A - M #Creating a Center matrix by subtracting A matrix with its means M by column
               # so, Center matrix will store the difference between each object and the mean value
               # by doing this, the M means become the origin of x-y-z coordinates

CovA = np.cov(Center.T) #Calculate the covariance matrix between 3 features (aka 3 columns)
                        #Covariance is a measure of joint variability between factors (do bien thien cung nhau, thuan hay nghich)
                        #the numpy.cov() of python help to calculate the covariance matrix
                        #since np.cov() considers rows as features and columns as objects, we have to transpose Center matrix

eigval,eigvect = eig(CovA) #Do the eigendecompostion for CovA matrix

Project_Center = Center.dot(eigvect[:,:2]) #To project a matrix with eigenvectos, using Matrix.Eigenvectors
                                           #Since we retain only 2 components, use eigvet[:,:2]
                                           #Center(100x3) @ eigvect(3x2) => Project_Center(100x2)

Project_Center now has smaller size (100x2) compared to origninal matrix A (100x3)

Approx_reconstruct_center_matrix = Project_Center(100x2) @ eigvect_transpose(2x3) => matrix (100x3)


#--------------------------------------------------------------------#
#------------------Use sklearn library-------------------------------#
#--------------------------------------------------------------------#

from sklearn.decomposition import PCA #By default, PCA() from sklearn will use SVD model to decompose covariance matrix

#   https://stackoverflow.com/questions/32857029/python-scikit-learn-pca-explained-variance-ratio-cutoff
#   - The pca.explained_variance_ratio_ returns a vector of the variance explained by each dimension.
#   - The pca.explained_variance_ratio_[i] gives the variance explained solely by the i+1st dimension.
#   - The pca.explained_variance_ratio_.cumsum() will return a vector x 
#     such that x[i] returns the cumulative variance explained by the first i+1 dimensions.

#   (1) PCA().components_: Chuyển vị của ma trận vectơ riêng EigenVectors.T
#   (2) PCA().explained_variance_: Các giá trị riêng
#   (3) PCA().explained_variance_ratio_: Tỷ lệ phương sai so với dữ liệu gốc
#   (4) Hàm numpy.cumsum()

pca_stud12f = PCA().fit(df_stud12f) #The PCA() function is to create a PCA object
                                    # the .fit(stud12f) method is to use data stud12f to train the PCA model
                                    # the results will be a trained PCA model, stored in pca_stud12f variable

#####-----Choose the number of components to retain---------------####

                     METHOD 1: Elbow Method
print('ELBOW Method: choose k based on the rupture point of cumulative variance curve')
plt.figure(figsize=(8,8))
plt.plot(np.cumsum(pca_stud12f.explained_variance_ratio_), marker='o') #Plot the cumsum of variance ratio upto k-th component
plt.title('Cumulative variance curve of df_stud12f', color='brown', size='20')
plt.xlabel('Features',size=15)
plt.ylabel('Cumulative variance ratio to each K component', size=15)
plt.xlim(0,len(df_stud12f.columns))
plt.grid(axis='y') #To create horizontal lines go accross each value appear on y axis
plt.show()

                    METHOD 2: set a threshold (80-90% is good), then find K
print('---------------------------------------------------------------------------')
print('METHOD 2: set a wanted threshold of cumulative variance ratio, then find K')
print('---------------------------------------------------------------------------')

threshold = 0.9
percent = threshold*100

pca_stud12f = PCA(threshold) #Create a PCA object with cumulative variance ratio = 0.9 = 90%
pca_stud12f.fit(df_stud12f)  #Apply the 0.9 PCA model into the df_stud12f

k = pca_stud12f.n_components_ #Assign the number of components relevant to 90%
var = sum(pca_stud12f.explained_variance_ratio_) #Sum all the variance_ratio of pca_stud12f up to k component

print('To achieve cumulative variance ratio >= %.2f%%'%percent+',the number of components k must be %.f'%k)

###------------Example with faces data---------------###

from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people(min_faces_per_person=60) #Load the data of 60 people's faces into variable faces

faces.images.shape: (1348, 62, 47)
faces.images[0].shape: (62, 47)
faces.data.shape: (1348, 2914)

#Here, there are 1348 people's faces
#Each person's face data is described by a numerical matrix shaped 62x47
#The image matrix (62x47) is then reshape to a serie with 2914 in length (62x47 = 2914)
#Containing 2914 features = columns = dimensions = components
#Therefore faces.data.shape is (1348x2914)
# so, each row/serie of 2914 values/columns is the image data of one person

pca_faces_150 = PCA(150).fit(faces.data) #apply 150-component PCA model to faces.data (1348,2914)

print('\npca_faces_150 n_components:',pca_faces_150.n_components_) 
#number of retained components = 150

print('\npca_faces_150 components:\n', pca_faces_150.components_) 
#normalized eigenvectors of covariance matrix (by rows or Vt)

print('\npca_faces_150 components shape:', pca_faces_150.components_.shape) 
#150 retained components/eigvects x 2914 rows

print('\npca_faces_150 explained variance:\n', pca_faces_150.explained_variance_) 
#sqrt(eigenvalues) of covariance matrix

print('\npca_faces_150 explained variance shape:\n', pca_faces_150.explained_variance_.shape) 
#A list of 150 explained variance values ~ 150 retained components

cum_var = sum(pca_faces_150.explained_variance_ratio_)
print('\nIf retain 150 components, we preserve %.2f%% of information'%(cum_var*100)) #===> preserve 93.59% of information

data_faces_150 = pca_faces_150.transform(faces.data) #Create reduced-dimension data, retained only 150 components/columns of 2914 original components
                                                     #Shape: 1348x150
df_faces_150 = pd.DataFrame(data_faces_150) #Convert output matrix into dataframe

data_faces_reconstruct = pca_faces_150.inverse_transform(data_faces_150) #Reconstruct approximate 1348x2914 faces data from 1348x150 reduced faces data
df_faces_reconstruct = pd.DataFrame(data_faces_reconstruct)

# Plot the results
fig, ax = plt.subplots(2, 10, figsize = (10, 2.5), 
                              subplot_kw  = {'xticks':[], 'yticks':[]},
                              gridspec_kw = dict(hspace = 0.1, wspace = 0.1))
for i in range(10):
   ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap = 'binary_r')
   ax[1, i].imshow(data_faces_reconstruct[i].reshape(62, 47),  cmap='binary_r')
   ax[0, 0].set_ylabel('full-dim\ninput')
   ax[1, 0].set_ylabel('150-dim\nreconstruction')

