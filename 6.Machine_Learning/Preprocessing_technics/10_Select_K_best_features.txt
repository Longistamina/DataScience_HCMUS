import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sbn

df = pd.read_.....

#---------------------------------------------------#
#--------------------- problems --------------------#
#---------------------------------------------------#

If we use too much features / inputs to train model
=> Can cause overfitting
=> Can make the training time longer

=> CHOOSE ESSENTIAL FEATURES ONLY

#---------------------------------------------------------------------------------------#
#--------------------- Use Select K best features Library ------------------------------#
#---------------------------------------------------------------------------------------#

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

# Apply SelectKBest class to extract all best features
k_best = SelectKBest(score_func=f_regression, k='all').fit(x, y)

#Create a dataframe Feature_name : Score
df_k_best_score = pd.DataFrame({
    'Feature_name':x.columns,
    'Score':k_best.scores_
})

#Sort in descending order
df_k_best_score = df_k_best_score.sort_values('Score', ascending=False).reset_index(drop = True)

df_k_best_score['Cumulative_Percentage'] = np.cumsum(df_k_best_score['Score']) / df_k_best_score['Score'].sum()

print(df_k_best_score)

#The bigger the score/weight that feature has, the more important that feature is
#Can choose the number of features where cummulative_percentage reaches 0.8 ~ 0.9 (or 80% ~ 90%)


#---------------------------------------------------------------#
#--------------------- score_func to select --------------------#
#---------------------------------------------------------------#

f_classif
=> ANOVA F-value between label/feature for classification tasks.

mutual_info_classif
=> Mutual information for a discrete target.

chi2
=> Chi-squared stats of non-negative features for classification tasks.

f_regression
=> F-value between label/feature for regression tasks.

mutual_info_regression
=> Mutual information for a continuous target.

SelectPercentile
=> Select features based on percentile of the highest scores.

SelectFpr
=> Select features based on a false positive rate test.

SelectFdr
=> Select features based on an estimated false discovery rate.

SelectFwe
=> Select features based on family-wise error rate.

GenericUnivariateSelect
=> Univariate feature selector with configurable mode.


#------------------------------------------------------------#
#---------------- Regression task ---------------------------#
#------------------------------------------------------------#

If the features are all numerical and continuous => f_regression

If the features are mixed between continous and categorical => mutual_info_regression


#------------------------------------------------------------#
#---------------- Classification task -----------------------#
#------------------------------------------------------------#

If the features are all numerical and continuous => f_classif

If the features are all categorical => chi2

If the features are mixed between continous and categorical => mutual_info_classif



#------------------------------------------------------------------------------------------------#
#--------------------- Use RandomForest to find important features ------------------------------#
#------------------------------------------------------------------------------------------------#
RandomForest module of sklearn in python can help calculate and select the most important features
=> Can use this to choose best features for a model beside SelectKBest module

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sbn

np.set_printoptions(suppress = True)

data = pd.read_...

x = data['col1', 'col2',...]
y = data['output']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)


from sklearn.ensemble import import RandomForestClassifier

forestC = RandomForestClassifier(n_estimators=100, crierion = 'gini', max_depth = None, min_samples_split = None)
# n_estimators is the number of trees generated by RandomForestClassifer (hence there will be 100 bootstrapped datasets as well)

forestC.fit(x_train, y_train)

forestC.feature_importances #To see the list of important features selected by random forest

df_features_scores = pd.DataFrame({
    'Feature':x_train.columns,
    'Score':forestC.feature_importances_
})

df_features_scores = df_features_scores.sort_values('Scores', ascending=False)
df_features_scores = df_features_scores.reset_index(drop = True)

df_features_scores['Cumulative_Percentage'] = np.cumsum(df_features_scores['Score']) / df_features_scores['Score'].sum()

print(df_features_scores)

#The bigger the score/weight that feature has, the more important that feature is
#Can choose the number of features where cummulative_percentage reaches 0.8 ~ 0.9 (or 80% ~ 90%)
