#-------------------------------------------------------------------------#
#------------------- BernoulliNB (Bernoulli Naive Bayes) -----------------#
#-------------------------------------------------------------------------#

Bernoulli Naive Bayes is a specialized form of the Naive Bayes classifier specifically designed for binary/boolean features. 
It's particularly effective for text classification tasks where you're concerned with whether a word appears in a document (presence/absence) rather than how many times it appears.

Key Characteristics
# Binary Feature Model: Assumes features are binary (0/1 values)
# Document Classification: Often used for document classification where features represent word occurrence (not frequency)
# Independence Assumption: Like all Naive Bayes models, it assumes feature independence given the class
# Distribution: Based on the multivariate Bernoulli distribution

How It Works
BernoulliNB models the probability of each feature being either present (1) or absent (0) in a given class. The formula is:
		P(x|y) = ∏ P(xᵢ|y)
Where:
# P(x|y) is the probability of observing feature vector x given class y
# P(xᵢ|y) is the probability of feature i being present/absent given class y

Differences from MultinomialNB
# BernoulliNB: Cares about whether a word is present or not (binary)
# MultinomialNB: Cares about word frequencies (counts)


#---------------------------------------------#
#------------------- Example -----------------#
#---------------------------------------------#

Let's classify emails as spam (1) or non-spam (0) based on the presence or absence of specific words.

*****************************
Step 1: Define our training data
Training documents:

Doc 1: "free money" (spam)
Doc 2: "free lunch" (spam)
Doc 3: "money back" (spam)
Doc 4: "business meeting" (non-spam)
Doc 5: "team lunch" (non-spam)
Doc 6: "meeting time" (non-spam)
**************************
Step 2: Create a vocabulary and binary feature representation
The vocabulary is the set of all unique words: {"free", "money", "lunch", "back", "business", "meeting", "team", "time"}
Binary matrix (1 = word present, 0 = word absent):

Document Class free money lunch back business meeting team time
Doc 1    1     1    1     0     0    0        0       0    0    
Doc 2    1     1    0     1     0    0        0       0    0    
Doc 3    1     0    1     0     1    0        0       0    0    
Doc 4    0     0    0     0     0    1        1       0    0    
Doc 5    0     0    0     1     0    0        0       1    0    
Doc 6    0     0    0     0     0    0        1       0    1
****************************************
Step 3: Calculate prior probabilities

P(spam) = 3/6 = 0.5
P(non-spam) = 3/6 = 0.5
******************************************
Step 4: Calculate conditional probabilities with Laplace smoothing
For a Bernoulli model, we need:

P(word present | class)
P(word absent | class)

Using Laplace smoothing to avoid 0 probability

	P(x_i = 1 | y) = (N_yi + α) / (N_y + |V|·α)

with α=1 and  |V|=2
(in BernoulliNB, the V is always = 2, not like MultinomialNB)

# For spam class (y=1):
	P(free=1|spam) = (2+1)/(3+2) = 3/5 = 0.6
	P(money=1|spam) = (2+1)/(3+2) = 3/5 = 0.6
	P(lunch=1|spam) = (1+1)/(3+2) = 2/5 = 0.4
	P(back=1|spam) = (1+1)/(3+2) = 2/5 = 0.4
	P(business=1|spam) = (0+1)/(3+2) = 1/5 = 0.2
	P(meeting=1|spam) = (0+1)/(3+2) = 1/5 = 0.2
	P(team=1|spam) = (0+1)/(3+2) = 1/5 = 0.2
	P(time=1|spam) = (0+1)/(3+2) = 1/5 = 0.2

	P(free=0|spam) = 1 - 0.6 = 0.4
	P(money=0|spam) = 1 - 0.6 = 0.4
	P(lunch=0|spam) = 1 - 0.4 = 0.6
	P(back=0|spam) = 1 - 0.4 = 0.6
	P(business=0|spam) = 1 - 0.2 = 0.8
	P(meeting=0|spam) = 1 - 0.2 = 0.8
	P(team=0|spam) = 1 - 0.2 = 0.8
	P(time=0|spam) = 1 - 0.2 = 0.8


# For non-spam class (y=0):
	P(free=1|non-spam) = (0+1)/(3+2) = 1/5 = 0.2
	P(money=1|non-spam) = (0+1)/(3+2) = 1/5 = 0.2
	P(lunch=1|non-spam) = (1+1)/(3+2) = 2/5 = 0.4
	P(back=1|non-spam) = (0+1)/(3+2) = 1/5 = 0.2
	P(business=1|non-spam) = (1+1)/(3+2) = 2/5 = 0.4
	P(meeting=1|non-spam) = (2+1)/(3+2) = 3/5 = 0.6
	P(team=1|non-spam) = (1+1)/(3+2) = 2/5 = 0.4
	P(time=1|non-spam) = (1+1)/(3+2) = 2/5 = 0.4

	P(free=0|non-spam) = 1 - 0.2 = 0.8
	P(money=0|non-spam) = 1 - 0.2 = 0.8
	P(lunch=0|non-spam) = 1 - 0.4 = 0.6
	P(back=0|non-spam) = 1 - 0.2 = 0.8
	P(business=0|non-spam) = 1 - 0.4 = 0.6
	P(meeting=0|non-spam) = 1 - 0.6 = 0.4
	P(team=0|non-spam) = 1 - 0.4 = 0.6
	P(time=0|non-spam) = 1 - 0.4 = 0.6

And the corresponding absence probabilities.

******************************************
Step 5: Classify a new document

Let's classify new_text = "free business meeting"

First, convert to binary feature vector:
	[1, 0, 0, 0, 1, 1, 0, 0] (features: free, money, lunch, back, business, meeting, team, time)

Calculate P(document|spam):
	P(doc|spam) = P(free=1|spam) × P(money=0|spam) × P(lunch=0|spam) × P(back=0|spam) × P(business=1|spam) × P(meeting=1|spam) × P(team=0|spam) × P(time=0|spam)
	P(doc|spam) = 0.6 × 0.4 × 0.6 × 0.6 × 0.2 × 0.2 × 0.8 × 0.8 = 0.00589824

Calculate P(document|non-spam):
	P(doc|non-spam) = P(free=1|non-spam) × P(money=0|non-spam) × P(lunch=0|non-spam) × P(back=0|non-spam) × P(business=1|non-spam) × P(meeting=1|non-spam) × P(team=0|non-spam)          × P(time=0|non-spam)

	P(doc|non-spam) = 0.2 × 0.8 × 0.6 × 0.8 × 0.4 × 0.6 × 0.6 × 0.6 = 0.01105920

Calculate P(spam.document) using Bayes' rule:
P(spam.doc) ∝ P(doc|spam) × P(spam) = 0.00589824 × 0.5 = 0.00294912
P(non-spam.doc) ∝ P(doc|non-spam) × P(non-spam) = 0.01105920 × 0.5 = 0.00552960

Since P(non-spam|doc) > P(spam|doc), we classify "free business meeting" as non-spam.
Final classification decision:

"free business meeting" is classified as non-spam with:

P(spam|document) ∝ 0.00294912
P(non-spam|document) ∝ 0.00552960

The model determined that despite containing the word "free" (common in spam), the presence of "business" and "meeting" (more common in non-spam) makes this more likely to be a legitimate email.
