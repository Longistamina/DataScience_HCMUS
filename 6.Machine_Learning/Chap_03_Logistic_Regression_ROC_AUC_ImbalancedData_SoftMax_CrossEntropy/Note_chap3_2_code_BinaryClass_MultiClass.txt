********************************************************
************** 2 output classes (binary) ***************
********************************************************

np.set_printoptions(suppress=True) #Suppress scientific notation
pd.set_option('display.max_rows', 5)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

df = pd.read_....()

x = df[['input1', 'input2', 'input3',...]]
y = df['output']

#If the input has categorical variables => OnehotEncoder, dummy,....
#If the input has unscaled variables => MinMaxScaler, StandardScaler,....

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=....)

from sklearn.linear_model import LogisticRegression

logreg_model = LogisticRegression().fit(x_train, y_train)

y_prob_train = logreg_model.predict_proba(x_train)
y_prob_test = logreg_model.predict_proba(x_test)

#logreg_model.predict_proba() returns an array of probability
# For example ([[0.7, 0.3],
	      [0.05, 0.95]])
#It means, the output predicted from x_test[0] is 70% belongs to 0, and 30% belongs to 1
#Meanwhile, the output predicted from x_test[1] is 5% belongs to 0, and 95% belongs to 1

y_pred_test = logreg_model.predict(x_test) #This returns the predicted output class to which the input x_test[i] belongs

logreg_model.score(x_test, y_test) #Accuracy score on test set
logreg_model.score(x_train, y_train) #Accuracy score on train set

#predict new input and plot
y_proba_new = logreg_model.predict_proba(x_new)

plt.plot(x_new, y_proba_new[:,0], label='Output 0', linestyle='r--')
plt.plot(x_new, y_proba_new[:,1], label='Output 1', linestyle='g-')
plt.xlabel('Input')
plt.ylabel('Output probability')
plt.legend()
plt.show()


#Visualize
#If X has only 2 features:
plt.scatter(x['col1'], x['col2'], c=y) #Draw true set
plt.scatter(x['col1'], x['col2'], c=logreg.predict(x)) #Draw predict value
plt.legend()
plt.show()


******************************************************************
************** Multiple output classes (>2 labels) ***************
******************************************************************

#df['output'] has many values like "class1", "class2", "class3"

#Can use LabelEnconder to encode the y df['output'] (this also helps trace back the original label)

np.set_printoptions(suppress=True) #Suppress scientific notation
pd.set_option('display.max_rows', 5)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

df = pd.read_....()


#Or can create numeric labels column manually like this:
output_dict = {'class1':1, 'class2':2, 'class3':3,....}
df['output_num'] = [output[class] for class in df['output']]

x = df['input1', 'input2', 'input3',...] #or use .drop()
y = df['output_num']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=....)

from sklearn.linear_model import LogisticRegression

logreg_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=300)
#Must choose multi_class='multinomial'
#For multinomial, choose solver='lbfgs' or solver='saga'
#max_iter is the loops of gradient descent to calculate. The default is max_iter=100. If cannot reach convergence, it will raise warnings, so need to increase max_iter

logreg_model.fit(x_train, y_train)

y_prob_train = logreg_model.predict_proba(x_train)
y_prob_test = logreg_model.predict_proba(x_test)

y_pred_test = logreg_model.predict(x_test)

#predict new input and plot
y_proba_new = logreg_model.predict_proba(x_new)

plt.plot(x_new, y_proba_new[:,0], label='Output 0', linestyle='r--')
plt.plot(x_new, y_proba_new[:,1], label='Output 1', linestyle='g-')
plt.plot(x_new, y_proba_new[:,2], label='Output 2', linestyle='g-')
....
plt.xlabel('Input')
plt.ylabel('Output probability')
plt.legend()
plt.show()
