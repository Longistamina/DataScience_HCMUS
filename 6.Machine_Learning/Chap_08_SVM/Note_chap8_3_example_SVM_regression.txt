SVM part 1 - Main ideas: https://www.youtube.com/watch?v=efR1C6CvhmE
SVM part 2 - Polynomial kernel: https://www.youtube.com/watch?v=Toet3EiSFcM
SVM part 3 - The Radial (RBF) kernel: https://www.youtube.com/watch?v=Qc5IyLW_hns

#-----------------------------------------------------------#
#------------------ Example SVM regression -----------------#
#-----------------------------------------------------------#

SVM for Regression (SVR)
For regression, SVM introduces an ε-insensitive loss function. 
Predictions within ε of the actual value incur no penalty, while those outside this tube are penalized linearly.


Let's work through a simple 1D example:
Data points: (1,3), (2,5), (3,7), (4,9), (5,11)

The hyperplane to find: 
	f(x) = w·x + b

such that: ∣y−f(x)∣ ≤ ε

#-------------------------------------------------#
         Step 1: Define the regression tube
#-------------------------------------------------#
Let's use ε = 0.5

So, our decision boundaries are:
	w·x + b - y = +ε = 0.5
	w·x + b - y = -ε = -0.5

So, we must find the w and b that satisfies:
	-0.5 < w·x + b - y < 0.5
	( |w·x + b - y| < 0.5 )

(for every xi and yi data points)


#---------------------------------------------------------#
	Step 2: Find the function f(x) = w·x + b
#---------------------------------------------------------#
Looking at our data, w = 2 and b = 1 seems to fit perfectly.


#----------------------------------------------------------------#
	Step 3: Verify the predictions fall within the ε-tube
#----------------------------------------------------------------#

For each point (x,y), we check if |f(x) - y| ≤ ε
(1,3): |2(1) + 1 - 3| = |0| ≤ 0.5 ✓
(2,5): |2(2) + 1 - 5| = |0| ≤ 0.5 ✓
(3,7): |2(3) + 1 - 7| = |0| ≤ 0.5 ✓
(4,9): |2(4) + 1 - 9| = |0| ≤ 0.5 ✓
(5,11): |2(5) + 1 - 11| = |0| ≤ 0.5 ✓


All points fit perfectly within our ε-tube, so w = 2 and b = 1 is our optimal solution for this example.
