SVM part 1 - Main ideas: https://www.youtube.com/watch?v=efR1C6CvhmE
SVM part 2 - Polynomial kernel: https://www.youtube.com/watch?v=Toet3EiSFcM
SVM part 3 - The Radial (RBF) kernel: https://www.youtube.com/watch?v=Qc5IyLW_hns


Kernel functions are the function that help project original data into a higher-dimension space

RBF: https://www.youtube.com/watch?v=Q7vT0--5VII

#------------------------------------------------------#
#------------------ Polynomial Kernel -----------------#
#------------------------------------------------------#

To apply a polynomial kernel to a dataset in Support Vector Machines (SVMs), you need to understand how the kernel function transforms the data.

The polynomial kernel function is defined as:
	K(x) = (Î³âŸ¨x@yâŸ© + r)^d
Where:
# x and y are your input vectors
# Î³ (gamma) is a scaling parameter
# r is a constant term (often 0 or 1)
# d is the degree of the polynomial
âŸ¨x@yâŸ© is the dot product between vectors x and y

(the dataset can have more observations, but here take x and y as 2 symbolized datapoints to form the formula)

For a polynomial kernel of degree d applied to N-dimensional data, the transformed feature space will have a dimensionality of:
	feature_space_dimension = (N+d)! / (d! * N!)

This represents the number of monomials of degree less than or equal to d with N variables.


****************** Example ****************************
Let's say you have 2-dimensional data points (N=2):
	x = (xâ‚, xâ‚‚)
	y = (yâ‚, yâ‚‚)

(the dataset can have more observations, but here take x and y as 2 symbolized datapoints to form the formula)

With a polynomial kernel of degree 2 (d=2), Î³=1, r=1:
	K(x, y) = (âŸ¨x@yâŸ© + 1)Â²
		= (xâ‚.yâ‚ + xâ‚‚.yâ‚‚ + 1)Â²

When expanded, this equals:
	(xâ‚.yâ‚ + xâ‚‚.yâ‚‚ + 1)Â² 
	= 1 + xâ‚Â²Â·yâ‚Â² + xâ‚‚Â²Â·yâ‚‚Â² + 2xâ‚xâ‚‚Â·yâ‚yâ‚‚ + 2xâ‚Â·yâ‚ + 2xâ‚‚Â·yâ‚‚
			     
	= (1, âˆš2Â·xâ‚, âˆš2Â·xâ‚‚, xâ‚Â², âˆš2Â·xâ‚Â·xâ‚‚, xâ‚‚Â²) @ (1, âˆš2Â·yâ‚, âˆš2Â·yâ‚‚, yâ‚Â², âˆš2Â·yâ‚Â·yâ‚‚, yâ‚‚Â²)  #Split into dot product of 2 vectors

This corresponds to the implicit feature mapping:
	Î¦(x) = (1, âˆš2Â·xâ‚, âˆš2Â·xâ‚‚, xâ‚Â², âˆš2Â·xâ‚Â·xâ‚‚, xâ‚‚Â²) #6 coordinates

feature_space_dimension = (N+d)! / (d! * N!) = (2+2)! / (2! * 2!) = 6

So, first 2-D   space with original features    x = (xâ‚, xâ‚‚)
now becomes 6-D space with transformed features x = (1, âˆš2Â·xâ‚, âˆš2Â·xâ‚‚, xâ‚Â², âˆš2Â·xâ‚Â·xâ‚‚, xâ‚‚Â²)

Then, apply the SVM to this new transformed features



#--------------------------------------------------#
#------------------ Radial Kernal -----------------#
#--------------------------------------------------#

The Radial Basis Function (RBF) Kernel is one of the most commonly used kernel functions. 
It allows SVMs to handle complex, non-linear classification problems by transforming the data into a higher-dimensional space (into infinite space)

Formula:
	K(x, y) = e^(-Î³ * âˆ¥x-yâˆ¥^2) 

# K(x, y) is the kernel function that computes similarity between two data points x and y
# âˆ¥x-yâˆ¥^2 is the squared Euclidean distance between the two data points x and y
# Î³ (gamma) > 0 is a hyperparameter that controls the influence of a single training example. A higher ð›¾ means closer points have more influence, 
while a lower ð›¾ means influence is spread out over a larger area.

=> The exponential function ensures that the result is always between 0 and 1, where 
# closed to 1 means the points are identical 
# closed to 0 means the points are very different.


**************** Why transforming into INFINITE space? *********************
Assume our data is 2D:
	x = (x1, x2)
	y = (y1, y2)
	
We have:
	K(x, y) = e^(-Î³ * âˆ¥x-yâˆ¥^2)   (Gaussian kernel)
	
		= e^(-Î³ * âˆ¥x-yâˆ¥^2)
		
		= e^(-Î³ * (âˆ¥xâˆ¥^2 -2xy + âˆ¥yâˆ¥^2))   #x and y here are vectors, so xy = transpose(x) @ y (a dot product)
		
		= e^(-Î³ * (âˆ¥xâˆ¥^2 + âˆ¥yâˆ¥^2) + Î³*2xy)
		
		= e^(-Î³ * (âˆ¥xâˆ¥^2 + âˆ¥yâˆ¥^2)) * e^(Î³*2xy)

Let's set Î³ = 1/2 
      and e^(-Î³ * (âˆ¥xâˆ¥^2 + âˆ¥yâˆ¥^2)) = S
      
=> K(x, y) = S * e^(xy)

Now let apply Taylor-Maclaurin expansion for e^(xy), we have

	e^(xy) = 1 + xy + [(xy)^2 / 2!] + [(xy)^3 / 3!] + ... + [(xy)^n / n!] + .....

Now let split into dot product:
	e^(xy) = 1 + xy + [(xy)^2 / 2!] + [(xy)^3 / 3!] + ... + [(xy)^n / n!] + .....
	       
	       = (1, x, (x^2)/âˆš2!, (x^3)/âˆš3!,...(x^n)/âˆšn!) @ (1, y, (y^2)/âˆš2!, (y^3)/âˆš3!,...(y^n)/âˆšn!)


So, the original x now becomes (1, x, (x^2)/âˆš2!, (x^3)/âˆš3!,...(x^n)/âˆšn!)
(if n going to infinity, then we have infinite dimension)


So, take Î³ = 0.5
	 x = (1,2)
         y = (3,4)
=> âˆ¥x-yâˆ¥^2 = (1-3)^2 + (2-4)^2 = 8

=> K(x,y) = e^(-0.5 * 8) = 0.01831563888 

(0.01831563888 describes the relationship between x and y in the infinite dimension)
# closed to 1 means the points are identical 
# closed to 0 means the points are very different.


#----------------------------------------------------------------------#
#------------------ Best thing about kernel functions -----------------#
#----------------------------------------------------------------------#
For both polynomial kernel and radial kernel:
	K(x) = (Î³âŸ¨x@yâŸ© + r)^d
	K(x, y) = e^(-Î³ * âˆ¥x-yâˆ¥^2)

SVM only uses these formulas to calculate the relationship between x and y in the higher dimension or infinite dimension,
without creating a higer-dimeinsion inputs like explaining above (or like Polynomial Regression)

=> Reduce the burden of computing, enhance the training time
