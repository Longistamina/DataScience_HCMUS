#-------------------------------------------------------------------------------------------------------------------#
#-----------------------------Polynomial Regression (Supervised Learning)-------------------------------------------#
#-------------------------------------------------------------------------------------------------------------------#

*******************************************
************** Simple Polynomial **********
*******************************************


Hoi quy da thuc

This is a non-linear regression

Denpends on the "shape" of the dataset, we will choose the order of the polynomial (generally <= 4)

2nd order: y_pred = a1*X + a2*X^2 + b
3rd order: y_pred = a1*x + a2*X^2 + a3*X^3 + b
.....

x = df['input']
y = df['output']

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

polyf = PolynomialFeatures(degree=2) #2nd order polynomial

x_pf = pf.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_pf, y, test_size=0.2, random_state=0)

lr_model = LinearRegression() #create model
lr_model.fit(x_train, y_train) #train model with x_train and y_train

y_pred_train = lr_model.predict(x_train) #Let model predict outputs from the train set
#compare y_pred_train and y_train to see the errors between them

y_pred_test = lr_model.predict(x_test) #Let model predict outputs from the test set
#compare y_pred_test and y_test to see the errors between them

same for MSE, MAE, R^2, intercept, coef,...
plots also same

*******************************************
************** Multi Polynomial ***********
*******************************************

x = df[['input1', 'input2', 'input3', 'input4',...]]
y = df['output']

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

polyf = PolynomialFeatures(degree=2) #2nd order polynomial

x_pf = pf.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x_pf, y, test_size=0.2, random_state=0)

lr_model = LinearRegression() #create model
lr_model.fit(x_train, y_train) #train model with x_train and y_train

y_pred_train = lr_model.predict(x_train) #Let model predict outputs from the train set
#compare y_pred_train and y_train to see the errors between them

y_pred_test = lr_model.predict(x_test) #Let model predict outputs from the test set
#compare y_pred_test and y_test to see the errors between them

same for MSE, MAE, R^2, intercept, coef,...
plots also same


*********************************************
************* Visualization *****************
*********************************************
x_draw = np.linspace(min(x_test_pf[:,1]),max(x_test_pf[:,1]),60)
x_draw_pf = PolynomialFeatures(degree=3).fit_transform(x_draw.reshape(-1,1))

sbn.lineplot(x=x_draw, y=linreg_pf.predict(x_draw_pf))
sbn.scatterplot(x = x_test_pf[:,1], y = y_test_pf)
plt.show()



***********************************************
************** NOTES for Polynomial ***********
***********************************************

Must apply the same degree = n for all input columns/variables

in case, we choose polynomial degree = n
then build model polynomial reg model with this degree = n

if the R^2 on test set < 0
=> The model has been OVERFITTED
=> must reduce the degree n

if the R^2 only nearly equal 50-60% (not high)
=> may be UNDERFITTED
=> increase degree n, or add more input variables (columns), or more observations (rows)


